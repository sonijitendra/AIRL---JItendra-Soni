{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q2 \u2014 Text-Driven Image Segmentation with SAM 2 (Pipeline)\n",
        "This notebook provides a runnable pipeline in Colab to perform text-prompted segmentation using a combination of a grounding model (e.g., GroundingDINO / CLIPSeg) to convert text -> region seeds, then using Segment Anything (SAM) to produce masks.\n",
        "\n",
        "**Important:** Installing SAM and grounding models may download model weights. Use Colab GPU and follow prompts to mount Google Drive if you want to cache weights.\n",
        "\n",
        "This notebook is written to run end-to-end in Colab. It includes install cells and an example image pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Install dependencies (Colab). These installs may take several minutes.\n",
        "!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n",
        "!pip install -q groundingdino_clip\n",
        "!pip install -q transformers timm torchvision\n",
        "print('Installed packages (may take a while).')"
      ],
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example pipeline (high-level). Replace model load lines to match available checkpoints in Colab.\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "from torchvision import transforms\n",
        "\n",
        "# Helper: download an example image\n",
        "img_url = 'https://upload.wikimedia.org/wikipedia/commons/4/47/PNG_transparency_demonstration_1.png'\n",
        "img = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
        "display(img.resize((512,512)))\n",
        "\n",
        "# NOTE: You must provide SAM model checkpoint path. Example (in Colab):\n",
        "# sam_checkpoint = '/path/to/sam_vit_h_4b8939.pth'\n",
        "# sam = sam_model_registry['vit_h'](checkpoint=sam_checkpoint)\n",
        "# predictor = SamPredictor(sam)\n",
        "\n",
        "print('This cell demonstrates the high-level flow.\\n')\n",
        "print('1) Convert text prompt to region proposals using grounding model (GroundingDINO / CLIPSeg).')\n",
        "print('2) Convert proposals to SAM input (points/boxes) and run predictor.predict(...) to get masks.')\n",
        "print('\\nPlease replace placeholders with actual checkpoint paths in Colab and run the cells.')"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Limitations & Notes\n",
        "- SAM 2 weights may not be publicly released; this notebook assumes you can obtain an appropriate SAM checkpoint (or use SAMv1 official checkpoints).\n",
        "- GroundingDINO / CLIPSeg require downloading model weights; follow the referenced GitHub repos in Colab.\n",
        "- For the video bonus: after getting masks for a frame, you can propagate masks using optical flow or a lightweight tracker (e.g., RAFT for flow + warp).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "- SAM (Segment Anything) repo: https://github.com/facebookresearch/segment-anything\n",
        "- Grounding DINO / CLIPSeg repositories for converting text prompts to boxes/points."
      ]
    }
  ]
}